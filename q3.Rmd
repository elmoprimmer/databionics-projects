---
title: "Q3"
output: html_notebook
---
*requirements*
```{r setup, include=FALSE}
library(rgl)
#library(rglwidget) #does not show any pictures in Rmarkdown if rglwidget()  is called after rgl
setupKnitr()
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message=FALSE,
                      webgl = TRUE,
                      dpi=100,
                      fig.width = 7, 
                      fig.height = 7,
                      fig.align='left',rgl.closewindows=TRUE
                      )
require(Umatrix)
require(FCPS)
require(DataVisualizations)
require(ProjectionBasedClustering)
require(GeneralizedUmatrix)
require(stats)
require(caret)
knitr::knit_hooks$set(webgl = hook_webgl)
```


*Import data*

```{r}

file_path <- "WisconsinDiagnosislrn.sec"
data <- read.table(file_path, sep = "\t", header = FALSE, comment.char = "%", skip = 1)

column_names <- c("Key", "Mean_radius", "Mean_texture", "Mean_perimeter", "Mean_area", "Mean_smoothness", "Mean_compactness", "Mean_concavity", "Mean_concave_points", "Mean_symmetry", "Mean_fractal_dimension", 
                  "SE_radius", "SE_texture", "SE_perimeter", "SE_area", "SE_smoothness", "SE_compactness", "SE_concavity", "SE_concave_points", "SE_symmetry", "SE_fractal_dimension", 
                  "Worst_radius", "Worst_texture", "Worst_perimeter", "Worst_area", "Worst_smoothness", "Worst_compactness", "Worst_concavity", "Worst_concave_points", "Worst_symmetry", "Worst_fractal_dimension")
colnames(data) <- column_names

head(data)


```



*Scaling and other preprocessing* 
```{r}

scale2 <- function(x) {
  #Scale all columns, [0,1]
  return ((x - min(x)) / (max(x) - min(x)))
}

data.sc2 <- as.data.frame(lapply(data, function(x) {
  if (is.numeric(x)) {
    return(scale2(x))
  } else {
    return(x)
  }
}))

data.sc2 = as.matrix(data.sc2)
data.sc = scale(data)

pca_result <- prcomp(data[, -1], scale. = TRUE)
pca_transformed_data <- pca_result$x
data.pca <- as.matrix(cbind(data[, 1], pca_transformed_data))

data.current <- data.sc2 ## Using Scale2() data now
```
*Comments*
We tested scaling the data with:
- the built in scale() function that sets values approx between -2,2
- a simple scaling function scale2() that scales data to be between 0,1 (Intuitively this works better as for euclidian distances) 
- a quick PCA, to also help with correlation


```{r}
MDplot(data.current)+ggplot2::ggtitle(
  'Is this a appropriate normalizations?')

cc=cor(data.current)
diag(cc)=0
DataVisualizations::Pixelmatrix(cc,
      main = 'Pearson Correlation')
```

*COmments*
Many features are extremely correlated, we need to do something about them. We try PCA and removing features with correlation coefficient > cutoff.


```{r}
OtherDistance=as.matrix(parallelDist::parallelDist(
  data.current,method = 'geodesic')
  )
OutputDim=50
Trans=ProjectionBasedClustering::MDS(
  DataOrDistances = OtherDistance,
  OutputDimension = OutputDim)$ProjectedPoints

DataVisualizations::Sheparddiagram(OtherDistance,
  as.matrix(dist(Trans)),main = 
  'Decide if relationship between distances is appropriate'
  )

V=DataVisualizations::ShepardDensityScatter(OtherDistance,
  as.matrix(dist(Trans)),main = 
  'Decide if relationship between distances is appropriate'
  )
```
*Comments*
Relationship appears appropriate. Values form a near straight line.

```{r}
high_cor_features <- findCorrelation(cc, cutoff = 0.75)
data.without_correlated <- data.current[, -high_cor_features]
data.without_correlated <- data.without_correlated[,-1]
```

*Comments*
- Remove 1 of each pair of features with correlation > 0.75
- This leaves us with 12 features, which seems appropriate for approx 600 data-points.


```{r}
MDplot(data.without_correlated)+ggplot2::ggtitle(
  'Is this a appropriate normalizations?')

cc=cor(data.without_correlated)
diag(cc)=0
DataVisualizations::Pixelmatrix(cc,
      main = 'Pearson Correlation')
```
*Comments*
New correlation analysis looks better.

```{r}
OtherDistance=as.matrix(parallelDist::parallelDist(
  data.without_correlated,method = 'geodesic')
  )
OutputDim=50
Trans=ProjectionBasedClustering::MDS(
  DataOrDistances = OtherDistance,
  OutputDimension = OutputDim)$ProjectedPoints

DataVisualizations::Sheparddiagram(OtherDistance,
  as.matrix(dist(Trans)),main = 
  'Decide if relationship between distances is appropriate'
  )

V=DataVisualizations::ShepardDensityScatter(OtherDistance,
  as.matrix(dist(Trans)),main = 
  'Decide if relationship between distances is appropriate'
  )
```


```{r echo = T, results = 'hide'}

res = esomTrain(Data = data.without_correlated,
                Lines = 60,
                Columns = 100,
                Epochs = 100,
                Toroid = T,
                NeighbourhoodFunction = "mexicanhat",
                StartLearningRate = 0.7,
                EndLearningRate = 0.01,
                StartRadius = 40,
                EndRadius = 4,
                NeighbourhoodCooling = "linear",
                LearningRateCooling = "linear",
                UmatrixForEsom = T)

```


```{r, message=FALSE, webgl=TRUE}
UmatrixInformation = Umatrix::umatrixForEsom(
  res$Weights, 
  Columns = res$Columns,
  Lines = res$Lines,
  Toroid = T
)

GeneralizedUmatrix::plotTopographicMap(UmatrixInformation,
                                       res$BestMatches,
                                       Toroid = T

)
```



```{r}
Pmatrix = pmatrixForEsom(data.without_correlated,
                         res$Weights,
                         res$Lines,
                         res$Columns,
                         res$Toroid)

plotMatrix(Pmatrix, ColorStyle = "Pmatrix", Toroid = T)

```



```{r}
Ustarmatrix = ustarmatrixCalc(res$Umatrix, Pmatrix)
```
```{r}
plotMatrix(Ustarmatrix, res$BestMatches, Toroid = T)
```
*Discussion*
Visualization does not clearly depict clusters or structures. There is a slight valley visible down the middle, but separating BMU's into clusters would not be possible.







*Assessing model*
We import classes to visualize how well the datapoints are mapped.
```{r}
file_path <- "WisconsinDiagnosis.cls"
classes <- read.table(file_path, comment.char = "#", sep = "\t", header = FALSE)
classes <- classes[!grepl("^%", classes$V1), ]
colnames(classes) <- c("key", "Class")
head(classes)
```




```{r}
ggobj=ProjectionBasedClustering::PlotProjectedPoints(
  res$BestMatches[,-1],
  Cls = classes$Class,

  BMUorProjected = T,
  main = 'Plot of Neural map, marked are BMUs')
```
- Plot of BMU's shows that classes have been correctly placed in clusters, but there us next to no distinction or gaps between clusters. 

```{r}
plotMatrix(Ustarmatrix, Cls = classes$Class, res$BestMatches, Toroid = T)
```
Even with ground truth classes, their separation is not clear.



*Would you trust the predictions of the neural network from the previous task?*

I would be wary of overfitting for the neural network. With only less than 600 samples (even less when split into test and train) and 30 features, extremely few of the possible samples are covered. We suffer from the empty space phenomenon. Ideally we would have 100^30 samples (or 100^10 after removing some features), but we are comically far from this. Moreover, with our failure to project it to three dimensions with the U Matrix, it appears that the decision is complex, and it is very possible that the problem is not well represented by these samples.

However, we cannot be sure that the neural network does not actually reach the results in practice as it does on the test set. This would need to be further investigated, for example with more data.

